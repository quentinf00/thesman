@misc{KaRInSWOTCharacteristics,
  title = {{{KaRIn}} on {{SWOT}}: {{Characteristics}} of {{Near-Nadir Ka-Band Interferometric SAR Imagery}}},
  shorttitle = {{{KaRIn}} on {{SWOT}}},
  urldate = {2023-09-18},
  abstract = {The principal instrument of the NASA/CNES wide-swath altimetry mission Surface Water and Ocean Topography (SWOT) is the Ka-band Radar Interferometer (KaRIn), a bistatic synthetic aperture radar (SAR) system operating on near-nadir swaths on both sides of the satellite track. There are limited reports on backscattering from natural surfaces at this short wavelength and particular observation geometry. Near-field backscattering measurements on water, as well as the first interferometric airborne SAR acquisitions at Ka-band covering the 0.6 \textdegree -3.9 \textdegree{} incidence range of KaRIn, were therefore conducted. The experimental results confirm expected characteristics of near-nadir Ka-band interferometric SAR imagery, such as strong water/land radiometric contrast (typically in the order of 10 dB) and very high interferometric coherence on water.},
  howpublished = {https://ieeexplore.ieee.org/document/6553583/},
  langid = {american},
  file = {/home/q20febvr/Zotero/storage/JY5YLPJW/6553583.html}
}
@article{taburetDUACSDT2018252019,
  title = {{{DUACS DT2018}}: 25 Years of Reprocessed Sea Level Altimetry Products},
  shorttitle = {{{DUACS DT2018}}},
  author = {Taburet, Guillaume and {Sanchez-Roman}, Antonio and Ballarotta, Maxime and Pujol, Marie-Isabelle and Legeais, Jean-Fran{\c c}ois and Fournier, Florent and Faugere, Yannice and Dibarboure, Gerald},
  year = {2019},
  month = sep,
  journal = {Ocean Science},
  volume = {15},
  number = {5},
  pages = {1207--1224},
  publisher = {{Copernicus GmbH}},
  issn = {1812-0784},
  doi = {10.5194/os-15-1207-2019},
  urldate = {2023-06-17},
  abstract = {For more than 20 years, the multi-satellite Data Unification and Altimeter Combination System (DUACS) has been providing near-real-time (NRT) and delayed-time (DT) altimetry products. DUACS datasets range from along-track measurements to multi-mission sea level anomaly (SLA) and absolute dynamic topography (ADT) maps. The DUACS DT2018 ensemble of products is the most recent and major release. For this, 25 years of altimeter data have been reprocessed and are available through the Copernicus Marine Environment Monitoring Service (CMEMS) and the Copernicus Climate Change Service (C3S).  Several changes were implemented in DT2018 processing in order to improve the product quality. New altimetry standards and geophysical corrections were used, data selection was refined and optimal interpolation (OI) parameters were reviewed for global and regional map generation.  This paper describes the extensive assessment of DT2018 reprocessing. The error budget associated with DT2018 products at global and regional scales was defined and improvements on the previous version were quantified (DT2014; Pujol et al., 2016). DT2018 mesoscale errors were estimated using independent and in situ measurements. They have been reduced by nearly 3\&thinsp;\% to 4\&thinsp;\% for global and regional products compared to DT2014. This reduction is even greater in coastal areas (up to 10\&thinsp;\%) where it is directly linked to the geophysical corrections applied to DT2018 processing. The conclusions are very similar concerning geostrophic currents, for which error was globally reduced by around 5\&thinsp;\% and as much as 10\&thinsp;\% in coastal areas.},
  langid = {english},
  file = {/home/q20febvr/Zotero/storage/CVIE2R2V/Taburet et al. - 2019 - DUACS DT2018 25 years of reprocessed sea level al.pdf}
}

@article{ballarottaResolutionsOceanAltimetry2019,
  title = {On the Resolutions of Ocean Altimetry Maps},
  author = {Ballarotta, Maxime and Ubelmann, Cl{\'e}ment and Pujol, Marie-Isabelle and Taburet, Guillaume and Fournier, Florent and Legeais, Jean-Fran{\c c}ois and Faug{\`e}re, Yannice and Delepoulle, Antoine and Chelton, Dudley and Dibarboure, G{\'e}rald and Picot, Nicolas},
  year = {2019},
  month = aug,
  journal = {Ocean Science},
  volume = {15},
  number = {4},
  pages = {1091--1109},
  publisher = {{Copernicus GmbH}},
  issn = {1812-0784},
  doi = {10.5194/os-15-1091-2019},
  urldate = {2023-09-18},
  abstract = {The Data Unification and Altimeter Combination System (DUACS) produces sea level global and regional maps that serve oceanographic applications, climate forecasting centers, and geophysics and biology communities. These maps are generated using an optimal interpolation method applied to altimeter observations. They are provided on a global 1/4{$\circ\&$}thinsp;\texttimes\&thinsp;1/4{$\circ$} (longitude\&thinsp;\texttimes\&thinsp;latitude) and daily grid resolution framework (1/8{$\circ\&$}thinsp;\texttimes\&thinsp;1/8{$\circ$} longitude\&thinsp;\texttimes\&thinsp;latitude grid for the regional products) through the Copernicus Marine Environment Monitoring Service (CMEMS). Yet, the dynamical content of these maps does not have full 1/4{$\circ$} spatial and 1\&thinsp;d temporal resolutions due to the filtering properties of the optimal interpolation. In the present study, we estimate the effective spatial and temporal resolutions of the newly reprocessed delayed-time DUACS maps (a.k.a. DUACS-DT2018). Our approach is based on the ratio between the spectral content of the mapping error and the spectral content of independent true signals (along-track and tide gauge observations), also known as the noise-to-signal ratio. We found that the spatial resolution of the DUACS-DT2018 global maps based on sampling by three altimeters simultaneously ranges from {$\sim$}100\&thinsp;km wavelength at high latitude to {$\sim$}800\&thinsp;km wavelength in the equatorial band and the mean temporal resolution is {$\sim$}34\&thinsp;d. The mean effective spatial resolution at midlatitude is estimated to be {$\sim$}200\&thinsp;km. The mean effective spatial resolution is {$\sim$}130\&thinsp;km for the regional Mediterranean Sea and for the regional Black Sea products. An intercomparison with previous DUACS reprocessing systems (a.k.a., DUACS-DT2010 and DUACS-DT2014) highlights the progress of the system over the past 8 years, in particular a gain of resolution in highly turbulent regions. The same diagnostic applied to maps constructed with two altimeters and maps with three altimeters confirms a modest increase in resolving capabilities and accuracies in the DUACS maps with the number of missions.},
  langid = {english},
  file = {/home/q20febvr/Zotero/storage/JVM383RE/Ballarotta et al. - 2019 - On the resolutions of ocean altimetry maps.pdf}
}
@misc{EmpiricalCrossCalibrationCoherent,
  title = {Empirical {{Cross-Calibration}} of {{Coherent SWOT Errors Using External References}} and the {{Altimetry Constellation}}},
  urldate = {2023-09-18},
  abstract = {This paper gives an overview of an empirical cross-calibration technique developed for the Surface Water Ocean Topography mission (SWOT). The method is here used to detect and to mitigate two spatially coherent errors in SWOT topography data: the baseline roll error whose signature is linear across track, and the baseline length error whose signature is quadratic across track. Assuming that topography data are corrupted by coherent error signatures that we can model, we extract the signatures, and we empirically use the error estimates to correct SWOT data. The cross-calibration is tackled with a two-step scheme. The first step is to get local estimates over cross-calibration zones, and the second step is to perform a global interpolation of local error estimates and to mitigate the error everywhere. Three methods are used to get local error estimates: 1) we remove a static first guess reference such as a digital elevation model, 2) we exploit overlapping diamonds between SWOT swaths, and 3) we exploit overlapping segments with traditional pulse-limited altimetry sensors. Then, the along-track propagation is performed taking the local estimates as an input, and an optimal interpolator (1-D objective analysis) constrained with a priori statistical knowledge of the problem. The rationale of this paper is to assume that SWOT's scientific requirements are met on all errors but the ones being cross-calibrated. In other words, the algorithms presented in this paper are not needed at this stage of the mission definition, and they are able to deal with higher error levels (e.g., if hardware constraints are relaxed and replaced by additional ground processing). Even in our most pessimistic theoretical scenarios of baseline roll and baseline length errors (up to 70 cm RMS of uncorrected topography error), the cross-calibration algorithm reduces coherent errors to less than 2 cm (outer edges of the swath). Residual errors are subcentimetric for very low-frequency errors (e.g., orbital revolution). Sensitivity tests highlight the benefits of using additional pulse-limited altimeters and optimal inversion schemes when the problem is more difficult to solve (e.g., wavelengths of less than 1000 km), but also to provide a geographically homogeneous correction that cannot be obtained with SWOT's sampling alone.},
  howpublished = {https://ieeexplore.ieee.org/document/6087373/},
  langid = {american},
  file = {/home/q20febvr/Zotero/storage/PUDJN6LV/6087373.html}
}

@article{verrierAssessingImpactMultiple2017,
  title = {Assessing the Impact of Multiple Altimeter Missions and {{Argo}} in a Global Eddy-Permitting Data Assimilation System},
  author = {Verrier, Simon and Le Traon, Pierre-Yves and Remy, Elisabeth},
  year = {2017},
  month = dec,
  journal = {Ocean Science},
  volume = {13},
  number = {6},
  pages = {1077--1092},
  publisher = {{Copernicus GmbH}},
  issn = {1812-0784},
  doi = {10.5194/os-13-1077-2017},
  urldate = {2023-09-18},
  abstract = {A series of observing system simulation experiments (OSSEs) is carried out with a global data assimilation system at 1/4\textdegree{} resolution using simulated data derived from a 1/12\textdegree{} resolution free-run simulation. The objective is to not only quantify how well multiple altimeter missions and Argo profiling floats can constrain the global ocean analysis and 7-day forecast at 1/4\textdegree{} resolution but also to better understand the sensitivity of results to data assimilation techniques used in Mercator Ocean operational systems. The impact of multiple altimeter data is clearly evidenced even at a 1/4\textdegree{} resolution. Seven-day forecasts of sea level and ocean currents are significantly improved when moving from one altimeter to two altimeters not only on the sea level, but also on the 3-D thermohaline structure and currents. In high-eddy-energy regions, sea level and surface current 7-day forecast errors when assimilating one altimeter data set are respectively 20 and 45 \% of the error of the simulation without assimilation. Seven-day forecasts of sea level and ocean currents continue to be improved when moving from one altimeter to two altimeters with a relative error reduction of almost 30 \%. The addition of a third altimeter still improves the 7-day forecasts even at this medium 1/4\textdegree{} resolution and brings an additional relative error reduction of about 10 \%. The error level of the analysis with one altimeter is close to the 7-day forecast error level when two or three altimeter data sets are assimilated. Assimilating altimeter data also improves the representation of the 3-D ocean fields. The addition of Argo has a major impact on improving temperature and demonstrates the essential role of Argo together with altimetry in constraining a global data assimilation system. Salinity fields are only marginally improved. Results derived from these OSSEs are consistent with those derived from experiments with real data (observing system evaluations, OSEs) but they allow for more detailed characterisation of errors on analyses and 7-day forecasts. Both OSEs and OSSEs should be systematically used and intercompared to test data assimilation systems and quantify the impact of existing observing systems.},
  langid = {english},
  file = {/home/q20febvr/Zotero/storage/WYIZHCGN/Verrier et al. - 2017 - Assessing the impact of multiple altimeter mission.pdf}
}

@article{hamonImpactMultipleAltimeter2019,
  title = {Impact of {{Multiple Altimeter Data}} and {{Mean Dynamic Topography}} in a {{Global Analysis}} and {{Forecasting System}}},
  author = {Hamon, Mathieu and Greiner, Eric and Le Traon, Pierre-Yves and Remy, Elisabeth},
  year = {2019},
  month = jul,
  journal = {Journal of Atmospheric and Oceanic Technology},
  volume = {36},
  number = {7},
  pages = {1255--1266},
  issn = {0739-0572, 1520-0426},
  doi = {10.1175/JTECH-D-18-0236.1},
  urldate = {2023-09-18},
  abstract = {Satellite altimetry is one of the main sources of information used to constrain global ocean analysis and forecasting systems. In addition to in situ vertical temperature and salinity profiles and sea surface temperature (SST) data, sea level anomalies (SLA) from multiple altimeters are assimilated through the knowledge of a surface reference, the mean dynamic topography (MDT). The quality of analyses and forecasts mainly depends on the availability of SLA observations and on the accuracy of the MDT. A series of observing system evaluations (OSEs) were conducted to assess the relative importance of the number of assimilated altimeters and the accuracy of the MDT in a Mercator Ocean global 1/48 ocean data assimilation system. Dedicated tools were used to quantify impacts on analyzed and forecast sea surface height and temperature/ salinity in deeper layers. The study shows that a constellation of four altimeters associated with a precise MDT is required to adequately describe and predict upper-ocean circulation in a global 1/48 ocean data assimilation system. Compared to a one-altimeter configuration, a four-altimeter configuration reduces the mean forecast error by about 30\%, but the reduction can reach more than 80\% in western boundary current (WBC) regions. The use of the most recent MDT updates improves the accuracy of analyses and forecasts to the same extent as assimilating a fourth altimeter.},
  langid = {english},
  file = {/home/q20febvr/Zotero/storage/95A95LCT/Hamon et al. - 2019 - Impact of Multiple Altimeter Data and Mean Dynamic.pdf}
}

@article{hornikMultilayerFeedforwardNetworks1989,
  title = {Multilayer Feedforward Networks Are Universal Approximators},
  author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  year = {1989},
  month = jan,
  journal = {Neural Networks},
  volume = {2},
  number = {5},
  pages = {359--366},
  issn = {0893-6080},
  doi = {10.1016/0893-6080(89)90020-8},
  urldate = {2023-09-04},
  abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.},
  keywords = {Back-propagation networks,Feedforward networks,Mapping networks,Network representation capability,Sigma-Pi networks,Squashing functions,Stone-Weierstrass Theorem,Universal approximation},
  file = {/home/q20febvr/Zotero/storage/9DWKTA8Q/Hornik et al. - 1989 - Multilayer feedforward networks are universal appr.pdf;/home/q20febvr/Zotero/storage/FKKADE86/0893608089900208.html}
}
@inproceedings{heDeepResidualLearning2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2016},
  pages = {770--778},
  urldate = {2023-06-23},
  file = {/home/q20febvr/Zotero/storage/MJBPTAVY/He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf}
}
@inproceedings{ioffeBatchNormalizationAccelerating2015,
  title = {Batch {{Normalization}}: {{Accelerating Deep Network Training}} by {{Reducing Internal Covariate Shift}}},
  shorttitle = {Batch {{Normalization}}},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{Machine Learning}}},
  author = {Ioffe, Sergey and Szegedy, Christian},
  year = {2015},
  month = jun,
  pages = {448--456},
  publisher = {{PMLR}},
  issn = {1938-7228},
  urldate = {2023-09-18},
  abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a stateof-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82\% top-5 test error, exceeding the accuracy of human raters.},
  langid = {english},
  file = {/home/q20febvr/Zotero/storage/TL2TV429/Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf}
}
@misc{kingmaAdamMethodStochastic2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2017},
  month = jan,
  number = {arXiv:1412.6980},
  eprint = {1412.6980},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1412.6980},
  urldate = {2023-09-18},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning}
}
@incollection{summaLargeScaleMachineLearning2011,
  title = {Large-{{Scale Machine Learning}} with {{Stochastic Gradient Descent L\'eon Bottou}}},
  booktitle = {Statistical {{Learning}} and {{Data Science}}},
  editor = {Summa, Mireille Gettler and Bottou, Leon and Goldfarb, Bernard and Murtagh, Fionn and Pardoux, Catherine and Touati, Myriam},
  year = {2011},
  month = dec,
  edition = {0},
  pages = {33--42},
  publisher = {{Chapman and Hall/CRC}},
  doi = {10.1201/b11429-6},
  urldate = {2023-09-18},
  abstract = {During the last decade, the data sizes have grown faster than the speed of processors. In this context, the capabilities of statistical machine learning methods is limited by the computing time rather than the sample size. A more precise analysis uncovers qualitatively different tradeoffs for the case of small-scale and large-scale learning problems. The large-scale case involves the computational complexity of the underlying optimization algorithm in non-trivial ways. Unlikely optimization algorithms such as stochastic gradient descent show amazing performance for large-scale problems. In particular, second order stochastic gradient and averaged stochastic gradient are asymptotically efficient after a single pass on the training set.},
  isbn = {978-0-429-10768-9},
  langid = {english},
  file = {/home/q20febvr/Zotero/storage/6NLHAZSR/Summa et al. - 2011 - Large-Scale Machine Learning with Stochastic Gradi.pdf}
}
@article{chaiDeepLearningComputer2021,
  title = {Deep Learning in Computer Vision: {{A}} Critical Review of Emerging Techniques and Application Scenarios},
  shorttitle = {Deep Learning in Computer Vision},
  author = {Chai, Junyi and Zeng, Hao and Li, Anming and Ngai, Eric W. T.},
  year = {2021},
  month = dec,
  journal = {Machine Learning with Applications},
  volume = {6},
  pages = {100134},
  issn = {2666-8270},
  doi = {10.1016/j.mlwa.2021.100134},
  urldate = {2023-07-11},
  abstract = {Deep learning has been overwhelmingly successful in computer vision (CV), natural language processing, and video/speech recognition. In this paper, our focus is on CV. We provide a critical review of recent achievements in terms of techniques and applications. We identify eight emerging techniques, investigate their origins and updates, and finally emphasize their applications in four key scenarios, including recognition, visual tracking, semantic segmentation, and image restoration. We recognize three development stages in the past decade and emphasize research trends for future works. The summarizations, knowledge accumulations, and creations could benefit researchers in the academia and participators in the CV industries.},
  langid = {english},
  keywords = {Computer vision,Deep learning,Literature review,Machine learning},
  file = {/home/q20febvr/Zotero/storage/AWIM8DC4/Chai et al. - 2021 - Deep learning in computer vision A critical revie.pdf}
}
@misc{brownLanguageModelsAre2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year = {2020},
  month = jul,
  number = {arXiv:2005.14165},
  eprint = {2005.14165},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2005.14165},
  urldate = {2023-09-18},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language}
}
@inproceedings{dengImageNetLargescaleHierarchical2009,
  title = {{{ImageNet}}: {{A}} Large-Scale Hierarchical Image Database},
  shorttitle = {{{ImageNet}}},
  booktitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and {Fei-Fei}, Li},
  year = {2009},
  month = jun,
  pages = {248--255},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2009.5206848},
  abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called ``ImageNet'', a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500\textendash 1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
  keywords = {Explosions,Image databases,Image retrieval,Information retrieval,Internet,Large-scale systems,Multimedia databases,Ontologies,Robustness,Spine},
  file = {/home/q20febvr/Zotero/storage/85R9YDEZ/Deng et al. - 2009 - ImageNet A large-scale hierarchical image databas.pdf;/home/q20febvr/Zotero/storage/K3T9UV8I/5206848.html}
}
@misc{gaoPile800GBDataset2020,
  title = {The {{Pile}}: {{An 800GB Dataset}} of {{Diverse Text}} for {{Language Modeling}}},
  shorttitle = {The {{Pile}}},
  author = {Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and Presser, Shawn and Leahy, Connor},
  year = {2020},
  month = dec,
  number = {arXiv:2101.00027},
  eprint = {2101.00027},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2101.00027},
  urldate = {2023-09-18},
  abstract = {Recent work has demonstrated that increased training dataset diversity improves general cross-domain knowledge and downstream generalization capability for large-scale language models. With this in mind, we present \textbackslash textit\{the Pile\}: an 825 GiB English text corpus targeted at training large-scale language models. The Pile is constructed from 22 diverse high-quality subsets -- both existing and newly constructed -- many of which derive from academic or professional sources. Our evaluation of the untuned performance of GPT-2 and GPT-3 on the Pile shows that these models struggle on many of its components, such as academic writing. Conversely, models trained on the Pile improve significantly over both Raw CC and CC-100 on all components of the Pile, while improving performance on downstream evaluations. Through an in-depth exploratory analysis, we document potentially concerning aspects of the data for prospective users. We make publicly available the code used in its construction.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language}
}
@inproceedings{lecunHandwrittenDigitRecognition1989,
  title = {Handwritten {{Digit Recognition}} with a {{Back-Propagation Network}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {LeCun, Yann and Boser, Bernhard and Denker, John and Henderson, Donnie and Howard, R. and Hubbard, Wayne and Jackel, Lawrence},
  year = {1989},
  volume = {2},
  publisher = {{Morgan-Kaufmann}},
  urldate = {2023-09-18},
  abstract = {We present an application of back-propagation networks to hand(cid:173) written digit recognition. Minimal preprocessing of the data was  required, but architecture of the network was highly constrained  and specifically designed for the task. The input of the network  consists of normalized images of isolated digits. The method has  1 \% error rate and about a 9\% reject rate on zipcode digits provided  by the U.S. Postal Service.}
}
@inproceedings{ronnebergerUNetConvolutionalNetworks2015,
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  shorttitle = {U-{{Net}}},
  booktitle = {Medical {{Image Computing}} and {{Computer-Assisted Intervention}} \textendash{} {{MICCAI}} 2015},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
  year = {2015},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {234--241},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-24574-4_28},
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.},
  isbn = {978-3-319-24574-4},
  langid = {english},
  keywords = {Convolutional Layer,Data Augmentation,Deep Network,Ground Truth Segmentation,Training Image},
  file = {/home/q20febvr/Zotero/storage/5P565HI3/Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf}
}

@article{vaswaniAttentionAllYou,
  title = {Attention Is {{All}} You {{Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
  langid = {english},
  file = {/home/q20febvr/Zotero/storage/AQYLYH3Q/Vaswani et al. - Attention is All you Need.pdf}
}
@article{kimDeepVideoInpainting,
  title = {Deep {{Video Inpainting}}},
  author = {Kim, Dahun and Woo, Sanghyun and Lee, Joon-Young and Kweon, In So},
  abstract = {Video inpainting aims to fill spatio-temporal holes with plausible content in a video. Despite tremendous progress of deep neural networks for image inpainting, it is challenging to extend these methods to the video domain due to the additional time dimension. In this work, we propose a novel deep network architecture for fast video inpainting. Built upon an image-based encoder-decoder model, our framework is designed to collect and refine information from neighbor frames and synthesize still-unknown regions. At the same time, the output is enforced to be temporally consistent by a recurrent feedback and a temporal memory module. Compared with the state-of-the-art image inpainting algorithm, our method produces videos that are much more semantically correct and temporally smooth. In contrast to the prior video completion method which relies on time-consuming optimization, our method runs in near realtime while generating competitive video results. Finally, we applied our framework to video retargeting task, and obtain visually pleasing results.},
  langid = {english},
  file = {/home/q20febvr/Zotero/storage/WERWUU3L/Kim et al. - Deep Video Inpainting.pdf}
}
@misc{tianDeepLearningImage2020,
  title = {Deep {{Learning}} on {{Image Denoising}}: {{An}} Overview},
  shorttitle = {Deep {{Learning}} on {{Image Denoising}}},
  author = {Tian, Chunwei and Fei, Lunke and Zheng, Wenxian and Xu, Yong and Zuo, Wangmeng and Lin, Chia-Wen},
  year = {2020},
  month = aug,
  number = {arXiv:1912.13171},
  eprint = {1912.13171},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1912.13171},
  urldate = {2023-09-18},
  abstract = {Deep learning techniques have received much attention in the area of image denoising. However, there are substantial differences in the various types of deep learning methods dealing with image denoising. Specifically, discriminative learning based on deep learning can ably address the issue of Gaussian noise. Optimization models based on deep learning are effective in estimating the real noise. However, there has thus far been little related research to summarize the different deep learning techniques for image denoising. In this paper, we offer a comparative study of deep techniques in image denoising. We first classify the deep convolutional neural networks (CNNs) for additive white noisy images; the deep CNNs for real noisy images; the deep CNNs for blind denoising and the deep CNNs for hybrid noisy images, which represents the combination of noisy, blurred and low-resolution images. Then, we analyze the motivations and principles of the different types of deep learning methods. Next, we compare the state-of-the-art methods on public denoising datasets in terms of quantitative and qualitative analysis. Finally, we point out some potential challenges and directions of future research.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/home/q20febvr/Zotero/storage/CH8QMDJS/Tian et al. - 2020 - Deep Learning on Image Denoising An overview.pdf}
}
@article{fabletENDTOENDPHYSICSINFORMEDREPRESENTATION2021,
  title = {{{END-TO-END PHYSICS-INFORMED REPRESENTATION LEARNING FOR SATELLITE OCEAN REMOTE SENSING DATA}}: {{APPLICATIONS TO SATELLITE ALTIMETRY AND SEA SURFACE CURRENTS}}},
  shorttitle = {{{END-TO-END PHYSICS-INFORMED REPRESENTATION LEARNING FOR SATELLITE OCEAN REMOTE SENSING DATA}}},
  author = {Fablet, R. and Amar, M. M. and Febvre, Q. and Beauchamp, M. and Chapron, B.},
  year = {2021},
  month = jun,
  journal = {ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  volume = {V-3-2021},
  pages = {295--302},
  publisher = {{Copernicus GmbH}},
  issn = {2194-9042},
  doi = {10.5194/isprs-annals-V-3-2021-295-2021},
  urldate = {2023-06-19},
  abstract = {This paper addresses physics-informed deep learning schemes for satellite ocean remote sensing data. Such observation datasets are characterized by the irregular space-time sampling of the ocean surface due to sensors' characteristics and satellite orbits. With a focus on satellite altimetry, we show that end-to-end learning schemes based on variational formulations provide new means to explore and exploit such observation datasets. Through Observing System Simulation Experiments (OSSE) using numerical ocean simulations and real nadir and wide-swath altimeter sampling patterns, we demonstrate their relevance w.r.t. state-of-the-art and operational methods for space-time interpolation and short-term forecasting issues. We also stress and discuss how they could contribute to the design and calibration of ocean observing systems.},
  langid = {english},
  keywords = {adaptive sampling,data assimilation,end-to-end learning,inverse problems,satellite altimetry,sea surface dynamics,short-term forecasting,space oceanography,space-time interpolation,SWOT mission},
  file = {/home/q20febvr/Zotero/storage/C95ZYKXG/Fablet et al. - 2021 - END-TO-END PHYSICS-INFORMED REPRESENTATION LEARNIN.pdf}
}
@misc{ballarottaOceandatachallenges2020a_SSH_mapping_NATL60Material2020,
  title = {Ocean-Data-Challenges/2020a\_{{SSH}}\_mapping\_{{NATL60}}: {{Material}} for {{SSH}} Mapping Data Challenge},
  shorttitle = {Ocean-Data-Challenges/2020a\_{{SSH}}\_mapping\_{{NATL60}}},
  author = {Ballarotta, Maxime and Cosme, Emmanuel and Albert, Aur{\'e}lie},
  year = {2020},
  month = sep,
  doi = {10.5281/zenodo.4045400},
  urldate = {2023-09-18},
  abstract = {This repository contains codes and sample notebooks for downloading and processing the SSH mapping data challenge. The goal is to investigate how to best reconstruct sequences of Sea Surface Height (SSH) maps from partial satellite altimetry observations. This data challenge follows an Observation System Simulation Experiment framework: "Real" full SSH are from a numerical simulation with a realistic, high-resolution ocean circulation model: the reference simulation. Satellite observations are simulated by sampling the reference simulation based on realistic orbits of past, existing or future altimetry satellites. A baseline reconstruction method is provided (see below) and the practical goal of the challenge is to beat this baseline according to scores also described below and in Jupyter notebooks. The structure of this data challenge was to a large extent inspired by WeatherBench.},
  howpublished = {Zenodo},
  keywords = {altimetry,data challenge,ocean}
}
@misc{ballarottaOceandatachallenges2021a_SSH_mapping_OSEMaterial2021,
  title = {Ocean-Data-Challenges/2021a\_{{SSH}}\_mapping\_{{OSE}}: {{Material}} for {{SSH}} Mapping {{OSE}} Data Challenge},
  shorttitle = {Ocean-Data-Challenges/2021a\_{{SSH}}\_mapping\_{{OSE}}},
  author = {Ballarotta, Maxime and Guillou, Florian Le},
  year = {2021},
  month = sep,
  doi = {10.5281/zenodo.5511905},
  urldate = {2023-09-18},
  abstract = {This repository contains codes and sample notebooks for downloading and processing the SSH mapping data challenge. The goal is to investigate how to best reconstruct sequences of Sea Surface Height (SSH) maps from partial satellite altimetry observations. This data challenge follows an Observation System Experiment (OSE) framework: Satellite observations are from real sea surface height data from altimeter. The practical goal of the challenge is to investigate the best mapping method according to scores described below and in Jupyter notebooks.},
  howpublished = {Zenodo},
  file = {/home/q20febvr/Zotero/storage/6E3HZDZL/5511905.html}
}
