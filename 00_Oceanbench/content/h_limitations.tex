\section{Limitations} \label{sec:appendix_limitations}

\subsection{Framework Limitations}

While we have advertised \texttt{OceanBench} as a unifying framework that provides standardized processing steps that comply with domain-expert standards, we also highlight some potential limitations that could hinder its adoption for the wider community.

\textbf{Data Serving}. We provide a few datasets but we omit some of the original simulations. We found that the original simulations are terabytes/petabytes of data which becomes infeasible for most modest users (even with adequate CPU resources).  
This is very big problem and if we want to have a bigger impact, we may need to do more close collaborations with specified platforms like the Marine Data Store~\citep{MDSOCEANPHYSICS,MDSBIOGEOCHEMICAL,MDSOCEANPHYSICSENS,MDSINSITU,MDSWAVES,MDSALONGTRACK,MDSSSH} or the Climate Data Store~\citep{CDSREANALYSISSST,CDSOBSSST,CDSOBSOC,CDSOBSSSTENS}. Furthermore, there are many people that will not be able to do a lot of heavy duty research which indirectly favours institutions with adequate resources and marginalizing others. 
This is also problematic as those communities tend to be the ones who need the most support from the products of such frameworks.
We hope that leaving this open-source at least ensure that the knowledge is public.

\textbf{Framework Dependence}. The user has to "buy-into" the \texttt{hydra} framework to really take advantage of \texttt{OceanBench}. This adds a layer of abstraction and a new tool to learn. 
However, we designed the project so that high level usage does not require in-depth knowledge of the framework. 
In addition, we hope that, despite the complexity of project, users will appreciate the flexibility and extensibility of this framework.


\textbf{Lack of Metrics}. We do not provide the most exhaustive list of metrics available with the ocean community. In fact, we also believe that many of these metrics are often poor and do not effectively assess the goodness of our reconstructions. 
However, we do provide a platform that will hopefully be useful and easy to implement new and improved metrics.
Furthermore, having a wide range of metrics that are trusted across communities may help to improve the overall assessment of the different model performances~\cite{METRICSAVERAGE}.

\textbf{Limited ML Scope}. 
The framework does not support nor promote any machine learning methods and we lack any indication of comparing ML training and inference performance. 
However, we argue that a benchmark framework will allow us to effectively compare whichever ML methods are demonstratively the best which is a necessary preliminary step which offers users more flexibility in the long-run.

\textbf{Broad Oceans Application Scope}. 
We have targeted a broad ocean-application scope of state estimation.
However, there may be more urgent applications such as maritime monitoring, object tracking, and general ocean health.
However, we feel that many downstream applications require high-quality maps.
In addition, those downstream applications tend to be very complicated and are not always straightforward to apply ML under those instances.

\textbf{Full Pipeline Transparency}. We use a lot of different \texttt{xarray}-specific packages which have different design principles, assumptions and implementations. This may give the users an illusion of simplicity and transparency to real-world use. However, there are many underlying assumptions within each of the packages that may occlude a lot of design decisions.
Despite this limitation, we believe that being transparent about the processing steps and being consistent with the evaluation procedure will be beneficial for the ML research community.

\textbf{Scalability}. Scaling this to many terabytes or petabytes of data is easily the biggest limitation of the framework. In addition, we have only showcased demonstrations for 2D+T fields which are much less expensive than 3D+T fields.

\textbf{Deployability}. MLOPs has many wheels and it is not easy to integrate into existing systems. We offer no solutions to this. 
However, we believe that our framework is fully transparent in the assumptions and use cases which will facilitate some adoption into operational systems where they can further modify it for their use cases (see the evolution of \texttt{WeatherBench} and \texttt{ClimateBench}).

\textbf{Visualization Tools}.
We do not incorporate a high quality visualization tool that allows users to do pre- and post-analysis at a large scale. 
We do provide some simple visualization steps that are ML-relevant (see the GitHub repo) but it is very limited to ML standards.
One solution is to interface our pipeline with the source of many ocean datasets, e.g. Climate Data Store~\citep{CDSREANALYSISSST} or Marine Data Store~\citep{MDSOCEANPHYSICS}, then we can offset this task to them where they can offer better quality visualization tools.

\newpage
\subsection{Data Challenge Limitations}

We have showcased the SSH interpolation edition as a data challenge which could be helpful for real applications. 
However, in section~\ref{sec:problem_scope} we alluded to the greater task of general ocean state estimation which is more pertinent to the ocean sciences yet we don't address this directly with our data challenges.
Furthermore, we claim that the data challenges presented will help the ocean community with using ML for SSH interpolation.
Below, we outline some limitations which address these criticisms.

\textbf{Not the overall objective}. 
We recognized that we are far away from the actual reanalysis and forecasting goals of full state estimation. 
However, we argue that that is a rather ambitious challenge which will require a lot of interdisciplinary work across communities. 
In the meantime while we work towards that goal, operational centers could possibly improve their current products from ML-based techniques would would benefit downstream applications that deal directly with SSH.
Furthermore, SSH is an important variable in describing the full ocean state.
So a robust set of techniques that are able to solve the interpolation tasks could (in principal) be used to solve extra tasks.

\textbf{Small Region \& Period}.
We only feature a small region and period over the Gulfstream which is not representative of the different global regimes. 
This also does not take into account real things like \textit{data drift} which will inevitable occur in operational settings.
However, this is a dynamical regime and a well-studied area which does have some importance for specific communities and the results obtained offer some transferability to other dynamical regimes.
In addition, this area will have good coverage due to the new SWOT mission~\cite{SWOT} which will allow for further validation in the future.
Lastly, the area is small enough where the beginning stages for ML researchers is not overwhelmed with problems involving scale (even though we eventually want to arrive at global schemes).
We hope to extend our challenges to more relevant scenarios~\cite{MDSALONGTRACK}.

\textbf{Simulations versus Reanalysis}. We use simulations for the OSSE experiments instead of reanalysis. This is an open research question as it is unclear whether it's better to pretrain models on simulated ocean data or reanalysis ocean data. In future updates, we plan to add the reanalysis data to extend the challenge.

\textbf{Efficacy of OSSE Experiments}. We alluded to the idea that the OSSE experiments may not reflect the overarching goal of the user yet we provide more OSSE experiments than OSE experiments.
We acknowledged that it often does not coincide exactly with the OSE experiments which may give users a false sense of accomplishment and immediate transferability. 
However, we try to provide a framework where one could thoroughly experiment with the learning problem on OSSE configurations which can facilitate transfer learning to other domain-specific tasks.
We also anticipate that new \textit{real} SWOT data~\cite{SWOT} will start to become more available which will allow us to design better, realistic OSE experiments.

\textbf{Noise Characterization}.
Real data has noise to content with and we do not account for that within the SSH interpolation experiments.
The true noise we see in operational settings is structured and this would require more knowledge outside the scope of our teams expertise.
A more improved challenge would take these considerations into account.
We leave this as a future challenge for the community and we hope our platform can help facilitate this.

\textbf{Uncertainty Quantitification}.
We prefaced the problem statement with the idea of data assimilation which is the notion of \textit{state/parameter estimation under uncertain conditions and incomplete information}~\citep{DAGEOSCIENCE}.
However, we have not addressed any notion of uncertainty at all throughout the paper.
Uncertainty is difficult to quantify and we don't want to impose too many restrictions until we more sure about the efficacy of ML for easier problems.
However, to move the problem setting towards a more realistic setting, we can start to introduce metrics and additional requirements from future challenges, e.g. mean and standard deviation estimates or ensemble predictions.


\textbf{Operational Constraints}.
The real use case of SSH interpolation will involve global data and/or high-resolution data. 
This involves dealing with very high-dimensional spatiotemporal global state-space.
In practice, the necessity for the scalability of the method is of paramount importance.
However, there are also areas within the ML research community who are looking into many ways we can scale up physical models~\citep{VEROS,OCEANANIGANS} and machine learn models for geoscience tasks~\citep{SFNO}.
We anticipate that once a set of solutions are excepted by a community, the scalability will come later.




% \newpage
% \subsection{Dataset Limitations}



% \textbf{Limitations}. The scope of this is very specific to researchers who are interested in ML. It only serves as a baseline design. If one would like to scale, it would require much more engineering work to get everything to connect. This is already seen if we are to load datasets of PBs. In addition, this requires researchers to have access to considerably large machines to be able to run their own preprocessing schemes. We do our best to provide toy datasets of a modest size however, inevitably, one will probably need to work with larger and large datasets.

% \subsection*{Broader Impact}
% \label{sec:impact}

% The theme of interpolation is present in many applied communities with different names, e.g. kriging in ecology/hydrology, Optimal interpolation in oceanography, and Gaussian processes in statistics. We hope that this work bridges this gap between the communities and we invite other works to try to highlight concrete ways that machine learning and classic physics have commonalities.

% In the oceanography community in particular, we especially hope to see more adoption of machine learning methods for interpolation. DUACS is ultimately a closed-system so the wider scientific community does not have access to the algorithm. Our OI baseline hopefully unveils some of the finer details of the method. However, in general, the standard OI methods used in the applied community cannot keep up with the massive influxes of observations we receive. So this work is a first step in demonstrating that neural networks (in particular NerFs) are a viable, simpler, and scalable alternative.