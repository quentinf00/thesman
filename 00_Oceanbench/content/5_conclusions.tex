\section{Conclusions} \label{sec:conclusions}

The ocean community faces technological and algorithmic challenges to make the most of available observation and simulation datasets. 
In this context, recent studies evidence the critical role of ML schemes in reaching breakthroughs in our ability to monitor ocean dynamics for various space-time scales and processes. 
Nevertheless, domain-specific preprocessing steps and evaluation procedures slow down the uptake of ML toward real-world applications.

Through \texttt{OceanBench} framework, we embed domain-level requirements into the MLOPs considerations by building a flexible framework that adds this into the hyperparameter considerations for ML models. 
We proposed four challenges towards a ML-ready benchmarking suite for ocean observation challenges. 
We outlined the inner workings \texttt{OceanBench} and demonstrated its usefulness by recreating some preprocessing and analysis pipelines from a few data challenges involving SSH interpolation.
We firmly believe that the \texttt{OceanBench} platform is a crucial step to lowering the barrier of entry for new ML researchers interested in applying and developing their methods to relevant problems in the ocean sciences.

